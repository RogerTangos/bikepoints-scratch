{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from vars import DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cambridge-small'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_gpickle(\"./data/01_\" + DATASET + \".gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tag nodes with zipcodes, population, and total graph pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate zipcode_dict with nodes for each zipcode\n",
    "with open('./data/zipcode/zipcodes_boston-metro.csv', newline='') as f:\n",
    "    csvreader = csv.reader(f, delimiter=',')\n",
    "    next(csvreader) # skip header\n",
    "    for row in csvreader:\n",
    "        node_id, zipcode = row[0], row[1]\n",
    "        \n",
    "        if zipcode_dict.get(zipcode) is not None:\n",
    "            zipcode_dict[zipcode]['ids'] = zipcode_dict[zipcode]['ids'] + [node_id]\n",
    "        else:\n",
    "            zipcode_dict[zipcode] = {}\n",
    "            zipcode_dict[zipcode]['population'] = 0\n",
    "            zipcode_dict[zipcode]['ids'] = [node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate zipcode_dict with population for each zipcode\n",
    "# from https://worldpopulationreview.com/zips/massachusetts\n",
    "with open('./data/zipcode/population_by_zip_2020.csv', newline='') as f:\n",
    "    csvreader = csv.reader(f, delimiter=',')\n",
    "    next(csvreader) # skip header\n",
    "    for row in csvreader:\n",
    "        population, zipcode = int(row[3]), row[0].zfill(5) # 5 digit zipcode\n",
    "        \n",
    "        if zipcode_dict.get(zipcode):\n",
    "            zipcode_dict[zipcode]['population'] = population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total population visible in graph\n",
    "total_pop_dict = {}\n",
    "all_nodes_set = set(g.nodes())\n",
    "\n",
    "# populate a dictionary of zipcodes in use, and their populations\n",
    "for zipcode, inner_dict in zipcode_dict.items():\n",
    "    population = inner_dict['population']\n",
    "    nodes = set(inner_dict['ids'])\n",
    "    \n",
    "    # if the zipcode hasn't gotten its population yet, and the nodes intersect with the graph\n",
    "    if total_pop_dict.get(zipcode, True) and len(nodes.intersection(all_nodes_set)) > 0:\n",
    "        total_pop_dict[zipcode] = population\n",
    "\n",
    "# sum the values of the dictionary to get the total map population\n",
    "total_population = sum([v for k, v in total_pop_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag nodes with the zipcode, population, and total graph population\n",
    "for zipcode, inner_dict in zipcode_dict.items():\n",
    "    population = inner_dict['population']\n",
    "    nodes = inner_dict['ids']\n",
    "    \n",
    "    for node in nodes:\n",
    "        \n",
    "        # the zipcode dictionary may have nodes not contained in\n",
    "        # the particular dataset, so make sure to only try to tag those\n",
    "        if g.nodes.get(node):\n",
    "            g.nodes()[node]['zipcode'] = zipcode\n",
    "            g.nodes()[node]['population'] = population\n",
    "            g.nodes()[node]['total_graph_area_pop'] = total_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dijkstra from Central Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give every node an empty zipcode dictionary\n",
    "for node in g.nodes():\n",
    "    g.nodes()[node]['zipcodes'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in testing, cutoff of 1000 does not affect the boston graph. The max distance represented is 23 km.\n",
    "# a reasoanble cutoff for a larger graph might be 5000, representing 100 km.\n",
    "def record_lenghts_from_source(zipcode, node_id, weight='havlen', cutoff=5000):\n",
    "    \"\"\"\n",
    "    given a zipcode and a node_id, compute the distance from the node_id to\n",
    "    all other nodes on the graph. Then, record this information in nodes,\n",
    "    keyed to the zipcode.\n",
    "    \n",
    "    Returns lengths and paths, which may be convenient for analysis reasons.\n",
    "    \"\"\"\n",
    "    \n",
    "    lengths, paths = nx.single_source_dijkstra(g, node_id, weight=weight, cutoff=cutoff)\n",
    "    \n",
    "    for k, v in lenghts.items():\n",
    "        g.nodes()[k]['zipcodes'][zipcode] = {'pop_percent': 0,\n",
    "                                             'distance': v}\n",
    "\n",
    "    return lengths, paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networkx Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# betweenness - O^2\n",
    "# betweenness_subset = (nx.algorithms.centrality.betweenness_centrality(g))\n",
    "# central_nodes = [x[0] for x in list(betweenness_subset.items()) if x[1] > 0.2]\n",
    "# for node in central_nodes:\n",
    "#     g.nodes[node]['betweenness'] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# K-components: identifies likely subgraphs and subgraphs of subgraphs\n",
    "# not useful for this project, sadly. It mostly identifies playgrounds\n",
    "# and stadiums, since these are easy subsets\n",
    "# from networkx.algorithms import approximation as apxa\n",
    "# h = g.to_undirected() \n",
    "\n",
    "# foo = apxa.k_components(h, min_density=0.95)\n",
    "\n",
    "# for node in foo[1][0]:\n",
    "#     g.nodes[node]['k_1'] = True\n",
    "# for node in foo[2][0]:\n",
    "#     g.nodes[node]['k_2'] = True\n",
    "# for node in foo[2][1]:\n",
    "#     g.nodes[node]['k_2'] = True\n",
    "# for node in foo[2][2]:\n",
    "#     g.nodes[node]['k_2'] = True\n",
    "# for node in foo[3][0]:\n",
    "#     g.nodes[node]['k_2'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs forever. no result\n",
    "# from networkx.algorithms import approximation as apxa\n",
    "# apxa.maximum_independent_set(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # voterank - not useful. mostly just shows parks\n",
    "# central_nodes = nx.algorithms.centrality.voterank(g, number_of_nodes=10)\n",
    "# for node in central_nodes:\n",
    "#     g.nodes[node]['voterank'] = True\n",
    "\n",
    "\n",
    "# # betweenness - O^2\n",
    "# # betweenness_subset = (nx.algorithms.centrality.betweenness_centrality(g))\n",
    "# # central_nodes = [x[0] for x in list(betweenness_subset.items()) if x[1] > 0.2]\n",
    "# # for node in central_nodes:\n",
    "# #     g.nodes[node]['betweenness'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # closeness - again, not very useful.\n",
    "# closeness_subset = (nx.algorithms.centrality.closeness_centrality(g, distance='havlen'))\n",
    "\n",
    "# for node in central_nodes:\n",
    "#     g.nodes[node]['closeness'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(g, \"./data/02_\" + DATASET + \".gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikepoints",
   "language": "python",
   "name": "bikepoints"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
